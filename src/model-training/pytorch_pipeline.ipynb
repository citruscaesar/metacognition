{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msambhav-chandra\u001b[0m (\u001b[33mmetacognition\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 1.13.1\n",
      "PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192829337\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.7\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.5\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/cb/pytorch_1000000000000/work/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.13.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pytorch Version: {torch.__version__}\")\n",
    "print(torch.__config__.show())\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA = pathlib.Path(\"/media/sambhav/30AC4696AC46568E/datasets\")\n",
    "DATA = pathlib.Path.home() / \"datasets\"\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root = DATA.as_posix(),\n",
    "    #download = True,\n",
    "    train = True,\n",
    "    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((.5, ), (.5,))])\n",
    ")\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(\n",
    "    root = DATA.as_posix(),\n",
    "    train = False,\n",
    "    transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((.5, ), (.5,))])\n",
    ")\n",
    "\n",
    "#train_dataset.data.to(DEVICE)\n",
    "#train_dataset.targets.to(DEVICE)\n",
    "#test_dataset.data.to(DEVICE)\n",
    "#test_dataset.targets.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = torch.randint(1, len(train_dataset), (1,)).item()\n",
    "img, label = train_dataset.__getitem__(idx)\n",
    "plt.axis('off')\n",
    "plt.imshow(img.squeeze(), cmap = 'gray')\n",
    "print(img.shape)\n",
    "print(img.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "class Config():\n",
    "    def __init__(self, num_epochs, batch_size, learning_rate, loss_function, seed):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss_function = loss_function\n",
    "        self.seed = seed\n",
    "\n",
    "        self.num_workers = 4\n",
    "    \n",
    "    def log_config(self):\n",
    "        config_dictionary = {\n",
    "            \"num_epochs\" : self.num_epochs,\n",
    "            \"batch_size\" : self.batch_size,\n",
    "            \"learning_rate\" : self.learning_rate,\n",
    "            \"loss_function\": self.loss_function,\n",
    "            \"seed\": self.seed\n",
    "        }\n",
    "        return config_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Flatten(start_dim = 1, end_dim = -1)\n",
    "        self.linear1 = nn.Linear(28 * 28, 512)\n",
    "        self.linear2 = nn.Linear(512, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    num_batches = len(dataloader)\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(dataloader)):\n",
    "        data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        model.train()\n",
    "        prediction = model(data) \n",
    "\n",
    "        loss = criterion(prediction, targets) \n",
    "        epoch_loss += loss.item() / num_batches\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Train Loss: {epoch_loss}\")\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(model, dataloader, criterion):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        num_batches = len(dataloader)\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        model.eval()\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(dataloader)):\n",
    "            data, targets = data.to(DEVICE), targets.to(DEVICE)\n",
    "            prediction = model(data)\n",
    "            predicted_labels = prediction.argmax(1)\n",
    "\n",
    "            loss = criterion(prediction, targets)\n",
    "            epoch_loss += loss.item() / num_batches\n",
    "\n",
    "            correct += (prediction.argmax(1) == targets).int().sum().item() / len(dataloader.dataset)\n",
    "            accuracy = correct*100\n",
    "\n",
    "        print(f\"Test Loss: {epoch_loss}, Accuracy: {accuracy:>.2f}\\n\")\n",
    "        return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 69420\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "tinymodel = TinyModel().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "config = Config(25, 64, 3e-3, criterion._get_name(), SEED)\n",
    "\n",
    "optimizer = torch.optim.Adam(tinymodel.parameters(), lr = config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = config.batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = config.num_workers)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = config.batch_size,\n",
    "    shuffle = True,\n",
    "    num_workers = config.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project = \"fashion-mnist\", config = config.log_config()):\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "\n",
    "        wandb.watch(tinymodel, criterion, log = \"all\", log_freq=config.num_epochs)\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{config.num_epochs}\")\n",
    "        train_loss = train_one_epoch(tinymodel, train_loader, criterion, optimizer)\n",
    "        test_loss, accuracy = test_one_epoch(tinymodel, test_loader, criterion)\n",
    "\n",
    "        wandb.log({\"train/loss\": train_loss, \n",
    "                \"test/loss\": test_loss, \n",
    "                \"test/metric\": accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
