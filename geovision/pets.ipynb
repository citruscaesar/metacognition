{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\");\n",
    "\n",
    "# System Modules\n",
    "from pathlib import Path\n",
    "\n",
    "# General Purpose Libraries \n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v3 as iio\n",
    "\n",
    "# Paths and Directory Management\n",
    "from etl.pathfactory import PathFactory\n",
    "from etl.etl import reset_dir\n",
    "\n",
    "# Datasets and Datamodules\n",
    "from data.datamodules import ImageDatasetDataModule \n",
    "from datasets.oxfordiiitpets import OxfordIIITPetSegmentation\n",
    "\n",
    "# Transforms\n",
    "import torchvision.transforms.v2 as t\n",
    "\n",
    "# Models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.models.segmentation import fcn_resnet50 \n",
    "\n",
    "# Tasks\n",
    "from training.tasks import SegmentationTask \n",
    "\n",
    "# Loggers\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger, CSVLogger\n",
    "from lightning import seed_everything\n",
    "\n",
    "#Trainers\n",
    "from lightning import Trainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Type Hints\n",
    "from typing import Callable, Any, Optional, Literal\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "#%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_DIR = Path.cwd() / \"logs\"\n",
    "CHECKPOINTS_DIR = LOGS_DIR / \"checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UTILS\n",
    "#torchvision.datasets.OxfordIIITPet(pets_paths.path.parent, download = True)\n",
    "def plot_dataset_output(ds):\n",
    "    random_idx = torch.randint(len(ds), (1,)).item()\n",
    "    image, mask = ds.__getitem__(random_idx)\n",
    "\n",
    "    _, (img, fg, bg, unclass) = plt.subplots(1, 4, figsize = (10, 10))\n",
    "    img.imshow(image.permute(1, 2, 0));\n",
    "    fg.imshow(mask[0], cmap = \"gray\");\n",
    "    bg.imshow(mask[1], cmap = \"gray\");\n",
    "    unclass.imshow(mask[2], cmap = \"gray\");\n",
    "\n",
    "def plot_two_images(left, right):\n",
    "    _, (l, r) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "    l.imshow(left);\n",
    "    r.imshow(right);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_ckpt = ModelCheckpoint(\n",
    "    dirpath=CHECKPOINTS_DIR,\n",
    "    filename=\"epoch={epoch}-val_loss:{val_loss:2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    "    save_on_train_epoch_end=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir=LOGS_DIR,\n",
    "    name=\"csv\",\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "#wandb.finish()\n",
    "wandb_logger = WandbLogger(\n",
    "    save_dir=LOGS_DIR,\n",
    "    project=\"oxford-iiit-pet\",\n",
    "    log_model=False,\n",
    "    name=\"fcn-logging-test\",\n",
    "    version='1',\n",
    "    offline=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 69\n"
     ]
    }
   ],
   "source": [
    "experiment = {\n",
    "    \"dataset_name\": \"oxford-iiit-pet\",\n",
    "    \"task\": \"segmentation\",\n",
    "    \"random_seed\": 69,\n",
    "\n",
    "    \"eval_split\": .2,\n",
    "    \"batch_size\": 4,\n",
    "    \"grad_accum\": 1,\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    \"num_classes\": 3,\n",
    "    \"loss\": \"cross_entropy\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"momentum\": 0,\n",
    "    \"weight_decay\": 0,\n",
    "}\n",
    "seed_everything(experiment[\"random_seed\"])\n",
    "pets_paths = PathFactory(experiment[\"dataset_name\"], experiment[\"task\"])\n",
    "\n",
    "image_transform = t.Compose([\n",
    "    t.ToImage(),\n",
    "    t.ToDtype(torch.float32, scale = True),\n",
    "])\n",
    "\n",
    "mask_transform = t.Compose([\n",
    "    t.ToImage(),\n",
    "    t.ToDtype(torch.float32, scale = False),\n",
    "])\n",
    "\n",
    "augmentations = t.Compose([\n",
    "    t.RandomResizedCrop((256, 256), interpolation=0, antialias=False)\n",
    "])\n",
    "\n",
    "pets_dm = ImageDatasetDataModule(\n",
    "    root = pets_paths.path,\n",
    "    dataset_constructor=OxfordIIITPetSegmentation,\n",
    "    is_remote=False,\n",
    "    is_streaming=False,\n",
    "    image_transform=image_transform,\n",
    "    target_transform=mask_transform,\n",
    "    common_transform=augmentations,\n",
    "    **experiment\n",
    ")\n",
    "\n",
    "fcn = fcn_resnet50(\n",
    "    weights = None,\n",
    "    num_classes = 3,\n",
    "    weights_backbone = ResNet50_Weights.DEFAULT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#LOGS_DIR = Path.cwd() / \"logs\"\n",
    "#CHECKPOINTS_DIR = LOGS_DIR / \"checkpoints\"\n",
    "\n",
    "#reset_dir(LOGS_DIR)\n",
    "#reset_dir(CHECKPOINTS_DIR)\n",
    "\n",
    "last_ckpt_path = (CHECKPOINTS_DIR / \"last.ckpt\").as_posix() if (CHECKPOINTS_DIR / \"last.ckpt\").is_file() else None\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger = csv_logger,\n",
    "    callbacks = val_loss_ckpt,\n",
    "    #fast_dev_run=True,\n",
    "    limit_train_batches=.1,\n",
    "    limit_val_batches=.1,\n",
    "    limit_test_batches=.1,\n",
    "    max_epochs=5,\n",
    "    check_val_every_n_epoch=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                  | Type                      | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | model                 | FCN                       | 32.9 M\n",
      "1 | criterion             | CrossEntropyLoss          | 0     \n",
      "2 | val_metrics           | MetricCollection          | 0     \n",
      "3 | val_cohen_kappa       | MulticlassCohenKappa      | 0     \n",
      "4 | val_confusion_matrix  | MulticlassConfusionMatrix | 0     \n",
      "5 | test_metrics          | MetricCollection          | 0     \n",
      "6 | test_cohen_kappa      | MulticlassCohenKappa      | 0     \n",
      "7 | test_confusion_matrix | MulticlassConfusionMatrix | 0     \n",
      "--------------------------------------------------------------------\n",
      "32.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.9 M    Total params\n",
      "131.791   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sambhav/miniconda3/envs/dev/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 110/110 [00:45<00:00,  2.44it/s, v_num=1]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 110/110 [00:45<00:00,  2.44it/s, v_num=1]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model = SegmentationTask(fcn, **experiment),\n",
    "    datamodule = pets_dm,\n",
    "    ckpt_path = last_ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/sambhav/miniconda3/envs/dev/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation DataLoader 0: 100%|██████████| 37/37 [00:06<00:00,  5.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.40014639496803284    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val_macro_accuracy     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7460123300552368     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_macro_dice       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7606865167617798     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_macro_f1        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7606865167617798     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_macro_iou       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6453152894973755     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    val_micro_accuracy     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8455833196640015     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      val_micro_dice       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8455833196640015     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_micro_f1        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8455833196640015     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       val_micro_iou       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7324767112731934     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.40014639496803284   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val_macro_accuracy    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7460123300552368    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_macro_dice      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7606865167617798    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_macro_f1       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7606865167617798    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_macro_iou      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6453152894973755    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   val_micro_accuracy    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8455833196640015    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     val_micro_dice      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8455833196640015    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_micro_f1       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8455833196640015    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      val_micro_iou      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7324767112731934    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.40014639496803284,\n",
       "  'val_macro_accuracy': 0.7460123300552368,\n",
       "  'val_macro_dice': 0.7606865167617798,\n",
       "  'val_macro_f1': 0.7606865167617798,\n",
       "  'val_macro_iou': 0.6453152894973755,\n",
       "  'val_micro_accuracy': 0.8455833196640015,\n",
       "  'val_micro_dice': 0.8455833196640015,\n",
       "  'val_micro_f1': 0.8455833196640015,\n",
       "  'val_micro_iou': 0.7324767112731934}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(\n",
    "    model = SegmentationTask(fcn, **experiment),\n",
    "    datamodule = pets_dm,\n",
    "    ckpt_path=last_ckpt_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.test(\n",
    "    #model = SegmentationTask(fcn, **experiment),\n",
    "    #datamodule = pets_dm,\n",
    "    #ckpt_path = last_ckpt_path\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write Classification Report Function\n",
    "# TODO: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "pets_dm.setup(\"fit\")\n",
    "ds = pets_dm.val_dataset\n",
    "random_idx = torch.randint(len(ds), (1,)).item()\n",
    "image, mask = ds.__getitem__(random_idx)\n",
    "\n",
    "image = image.to(DEVICE)\n",
    "mask = mask.to(DEVICE)\n",
    "\n",
    "task = SegmentationTask.load_from_checkpoint(CHECKPOINTS_DIR/\"last.ckpt\", model = fcn)\n",
    "pred_mask = task.forward(image.unsqueeze(0))[\"out\"].detach().cpu().squeeze()\n",
    "mask = mask.cpu()\n",
    "\n",
    "#print(mask.shape)\n",
    "#print(pred_mask.shape)\n",
    "pred = torch.argmax(pred_mask, 0).to(torch.int64)\n",
    "mask = torch.argmax(mask, 0).to(torch.int64)\n",
    "\n",
    "plot_two_images(pred, mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
