{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%autoreload 2\n",
    "%dotenv ./env_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2 as t\n",
    "from typing import Callable, Optional, Literal, Any\n",
    "\n",
    "import sys; sys.path.append(\"../\") if \"../\" not in sys.path else None\n",
    "from viz.dataset_plots import plot_segmentation_samples\n",
    "\n",
    "import logging\n",
    "from lightning.pytorch.utilities import disable_possible_user_warnings # type: ignore\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "disable_possible_user_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tiled dataset @ [/home/sambhav/datasets/urban_footprint/inria.h5]\n"
     ]
    }
   ],
   "source": [
    "#from datasets.inria_speedtest import InriaImageFolder, InriaHDF5\n",
    "from datasets.inria import InriaLitData, InriaSegmentation, InriaHDF5\n",
    "inria_kwargs = {\n",
    "    \"root\" : Path.home() / \"datasets\" / \"urban_footprint\",\n",
    "    #\"root\" : Path.home() / \"datasets\" / \"urban_footprint\",\n",
    "    #\"shards\": Path.home() / \"shards\" / \"urban_footprint\",\n",
    "    #\"root\": Path.home() / \"shards\" / \"urban_footprint\",\n",
    "    \"test_split\": 0.2, \"val_split\": 0.2, \"random_seed\": 69,\n",
    "    \"tile_size\": (512, 512), \"tile_stride\": (512, 512),\n",
    "    \"split\": \"train\",\n",
    "    \"shard_size_in_mb\": 256 \n",
    "}\n",
    "#InriaSegmentation.write_to_litdata(**inria_kwargs)\n",
    "#ds = InriaHDF5(**inria_kwargs)\n",
    "InriaSegmentation.write_to_hdf(**inria_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_segmentation_samples(ds, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use Pytorch Profiler Here\n",
    "\n",
    "from segmentation_models_pytorch import Unet\n",
    "from geovision.datasets.inria import InriaImageFolder, InriaLitData, InriaHDF5\n",
    "\n",
    "inria_kwargs = {\n",
    "    \"root\" : Path.home() / \"datasets\" / \"urban_footprint\", \n",
    "    \"split\": \"train\",\n",
    "    \"shuffle\": True,\n",
    "    \"test_split\": 0.2, \"val_split\": 0.2, \"random_seed\": 69,\n",
    "    \"tile_size\": (512, 512), \"tile_stride\": (512, 512)\n",
    "}\n",
    "\n",
    "dl = DataLoader(\n",
    "    dataset = InriaHDF5(**inria_kwargs), \n",
    "    batch_size = 2, num_workers = 4, \n",
    "    pin_memory = True, prefetch_factor = 10 \n",
    ")\n",
    "unet = Unet(\"resnet18\", classes=2, encoder_weights=\"imagenet\") \n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "adam = torch.optim.Adam(unet.parameters(), lr = 1e-5)\n",
    "\n",
    "def train_one_epoch(dataloader: DataLoader, model: Module, criterion: Module, optimizer: Optimizer, limit_train_batches: Optional[int] = None):\n",
    "    if limit_train_batches is None:\n",
    "        limit_train_batches = len(dataloader)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for idx, batch in tqdm(enumerate(dataloader), total = limit_train_batches, unit = \"steps\"):\n",
    "        if idx >= limit_train_batches:\n",
    "            break\n",
    "            \n",
    "        images, masks = batch[0].to(device), batch[1].to(device)\n",
    "        preds = model(images) \n",
    "        loss = criterion(preds.argmax(1).to(torch.float32), masks.argmax(1).to(torch.float32)).mean()\n",
    "        loss.requires_grad_()\n",
    "        #print(f\"Step: {idx}, Loss: {loss}\")\n",
    "        #print(images.shape, images.dtype, images.min().item(), images.max().item())\n",
    "        #print(masks.shape, masks.dtype, masks.min().item(), masks.max().item())\n",
    "        #print(preds.shape, preds.dtype, preds.min().item(), preds.max().item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "train_one_epoch(dl, unet, loss_fn, adam, 500)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
