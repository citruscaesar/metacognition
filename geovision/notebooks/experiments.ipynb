{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%autoreload 2\n",
    "%dotenv ./env_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'type' and 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m; sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatamodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDatasetDataModule \n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minria\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InriaHDF5\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClassificationTask\n",
      "File \u001b[0;32m/notebooks/datasets/datamodules.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m### Custom Modules ###\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     validate_dir,\n\u001b[1;32m     12\u001b[0m     is_empty,\n\u001b[1;32m     13\u001b[0m     is_valid_remote, \n\u001b[1;32m     14\u001b[0m     is_valid_path,\n\u001b[1;32m     15\u001b[0m     get_local_path_from_remote,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m### Type Hints ###\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Literal, Optional, Callable\n",
      "File \u001b[0;32m/notebooks/etl/etl.py:26\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_path\u001b[39m(local_path: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     27\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m Path(local_path)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local_path\u001b[38;5;241m.\u001b[39mexists():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'type' and 'type'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms.v2 as t\n",
    "from lightning import Trainer\n",
    "\n",
    "import sys; sys.path.append(\"../\") if \"../\" not in sys.path else None\n",
    "from datasets.datamodules import ImageDatasetDataModule \n",
    "from datasets.inria import InriaHDF5\n",
    "from training.tasks import ClassificationTask\n",
    "from training.callbacks import (\n",
    "    setup_logger, setup_wandb_logger, setup_checkpoint, eval_callback\n",
    ")\n",
    "from etl.pathfactory import PathFactory\n",
    "from etl.etl import reset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "\n",
    "from lightning.pytorch.utilities import disable_possible_user_warnings # type: ignore\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "disable_possible_user_warnings()\n",
    "\n",
    "os.environ[\"WANDB_CONSOLE\"] = \"off\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = InriaHDF5 \n",
    "# MODEL = Unet\n",
    "experiment = {\n",
    "    \"name\": \"test_run\",\n",
    "    \"model_name\": \"unet\",\n",
    "    \"model_params\": {\n",
    "        \"encoder\": \"resnet18\",\n",
    "        \"decoder\": \"deconvolution\",\n",
    "        \"weights\": \"imagenet\",\n",
    "    },\n",
    "\n",
    "    \"dataset_name\": DATASET.NAME,\n",
    "    \"task\": DATASET.TASK,\n",
    "    \"num_classes\": DATASET.NUM_CLASSES,\n",
    "    \"class_names\": DATASET.CLASS_NAMES,\n",
    "\n",
    "    \"random_seed\": 69,\n",
    "\n",
    "    \"test_split\": 0.2,\n",
    "    \"val_split\": 0.2,\n",
    "    \"batch_size\": 4,\n",
    "    \"grad_accum\": 1,\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    \"loss\": \"cross_entropy\",\n",
    "    \"loss_params\": {\n",
    "        \"reduction\": \"mean\",\n",
    "    },\n",
    "\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"optimizer_params\": {\n",
    "        \"lr\": 1e-5,\n",
    "    },\n",
    "\n",
    "    \"monitor_metric\": \"iou\",\n",
    "    \"monitor_mode\": \"max\",\n",
    "\n",
    "    \"tile_size\": (512, 512),\n",
    "    \"tile_stride\": (512, 512),\n",
    "}\n",
    "PATHS = PathFactory(experiment[\"dataset_name\"], experiment[\"task\"])\n",
    "LOGS_DIR = PATHS.experiments_path / experiment[\"name\"]\n",
    "\n",
    "# NOTE: t.Normalize(DATASET.MEANS, DATASET.STD_DEVS),\n",
    "image_transform = t.Compose([t.ToImage(), t.ToDtype(torch.float32, scale=True)])\n",
    "mask_transform = t.Compose([t.ToImage(), t.ToDtype(torch.float32, scale=False)])\n",
    "#augmentations = t.Compose([t.RandomHorizontalFlip(0.5))\n",
    "augmentations = None\n",
    "\n",
    "datamodule = ImageDatasetDataModule(\n",
    "    root = PATHS.path,\n",
    "    is_remote = False,\n",
    "    is_streaming = False,\n",
    "    dataset_constructor = DATASET, \n",
    "    image_transform = image_transform,\n",
    "    target_transform = mask_transform,\n",
    "    common_transform = augmentations,\n",
    "    **experiment\n",
    ")\n",
    "display(datamodule)\n",
    "logger = setup_logger(PATHS.experiments_path, experiment[\"name\"])\n",
    "wandb_logger = setup_wandb_logger(PATHS.experiments_path, experiment[\"name\"])\n",
    "checkpoint = setup_checkpoint(Path(logger.log_dir, \"model_ckpts\"), experiment[\"monitor_metric\"], experiment[\"monitor_mode\"], \"all\") \n",
    "reset_dir(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "#from torchvision.models import resnet18, ResNet18_Weights\n",
    "#model = resnet18(weights = ResNet18_Weights.DEFAULT)\n",
    "#model.fc = torch.nn.Linear(512, experiment[\"num_classes\"])\n",
    "\n",
    "#from torchvision.models import alexnet, AlexNet_Weights\n",
    "#model = alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "#model = alexnet(weights=None)\n",
    "#model.classifier[-1] = torch.nn.Linear(4096, experiment.get(\"num_classes\", 10))\n",
    "\n",
    "from segmentation_models_pytorch import Unet\n",
    "model = Unet(experiment[\"model_params\"][\"encoder\"], classes=experiment[\"num_classes\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CKPT = checkpoint.best_model_path \n",
    "LAST_CKPT = checkpoint.last_model_path\n",
    "evaluation = eval_callback(experiment[\"task\"])\n",
    "trainer = Trainer(\n",
    "    #callbacks=[checkpoint],\n",
    "    enable_checkpointing=False,\n",
    "    logger = [logger],\n",
    "    enable_model_summary=False,\n",
    "    #fast_dev_run=True,\n",
    "    num_sanity_val_steps=0,\n",
    "    max_epochs=1,\n",
    "    #check_val_every_n_epoch=3, \n",
    "    #limit_train_batches=100,\n",
    "    #limit_val_batches=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#experiment[\"optimizer_params\"][\"lr\"] =  5e-6 \n",
    "trainer.fit(\n",
    "    model=ClassificationTask(model, **experiment),\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=LAST_CKPT if Path(LAST_CKPT).is_file() else None,\n",
    "    #verbose=False\n",
    ")\n",
    "\n",
    "#trainer.test(\n",
    "    #model=ClassificationTask(model, **experiment),\n",
    "    #datamodule=datamodule,\n",
    "    #ckpt_path=LAST_CKPT if Path(LAST_CKPT).is_file() else None,\n",
    "    #verbose = False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.evaluation import checkpoints_df, plot_checkpoints, plot_checkpoint_attribution\n",
    "plot_checkpoints(LOGS_DIR, experiment[\"monitor_metric\"], checkpoints_df(LOGS_DIR, experiment[\"monitor_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 11 \n",
    "step = 3024 \n",
    "split = \"best\"\n",
    "k = 25 \n",
    "plot_checkpoint_attribution(model, LOGS_DIR, PATHS.path, DATASET, epoch, step, split, k, **experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
