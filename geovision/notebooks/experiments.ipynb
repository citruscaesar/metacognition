{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext dotenv\n",
    "%autoreload 2\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torchvision.transforms.v2 as t\n",
    "from lightning import Trainer\n",
    "\n",
    "import sys; sys.path.append(\"../\") if \"../\" not in sys.path else None\n",
    "from datasets.datamodules import ImageDatasetDataModule \n",
    "from datasets.inria import InriaImageFolder, InriaHDF5 \n",
    "from training.tasks import ClassificationTask\n",
    "from training.utils import (\n",
    "    setup_logger, setup_wandb_logger, setup_checkpoint, #setup_eval\n",
    ")\n",
    "from training.callbacks import EvaluateSegmentation, EvaluateClassification\n",
    "from etl.pathfactory import PathFactory\n",
    "from etl.etl import reset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "\n",
    "from lightning.pytorch.utilities import disable_possible_user_warnings # type: ignore\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "disable_possible_user_warnings()\n",
    "\n",
    "os.environ[\"WANDB_CONSOLE\"] = \"off\"\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = InriaHDF5 \n",
    "# MODEL = Unet\n",
    "experiment = {\n",
    "    \"name\": \"hdf_speed_test\",\n",
    "    \"model_name\": \"unet\",\n",
    "    \"model_params\": {\n",
    "        \"encoder\": \"resnet18\",\n",
    "        \"decoder\": \"deconvolution\",\n",
    "        \"weights\": \"imagenet\",\n",
    "    },\n",
    "\n",
    "    \"dataset_name\": DATASET.NAME,\n",
    "    \"task\": DATASET.TASK,\n",
    "    \"num_classes\": DATASET.NUM_CLASSES,\n",
    "    \"class_names\": DATASET.CLASS_NAMES,\n",
    "\n",
    "    \"random_seed\": 69,\n",
    "\n",
    "    \"test_split\": 0.2,\n",
    "    \"val_split\": 0.1,\n",
    "    \"batch_size\": 8,\n",
    "    \"grad_accum\": 1,\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    \"loss\": \"binary_cross_entropy\",\n",
    "    \"loss_params\": {\n",
    "        \"reduction\": \"mean\",\n",
    "    },\n",
    "\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"optimizer_params\": {\n",
    "        \"lr\": 1e-5,\n",
    "    },\n",
    "\n",
    "    \"monitor_metric\": \"iou\",\n",
    "    \"monitor_mode\": \"max\",\n",
    "\n",
    "    \"tile_size\": (512, 512),\n",
    "    \"tile_stride\": (512, 512),\n",
    "}\n",
    "PATHS = PathFactory(experiment[\"dataset_name\"], experiment[\"task\"])\n",
    "LOGS_DIR = PATHS.experiments_path / experiment[\"name\"]\n",
    "\n",
    "# NOTE: t.Normalize(DATASET.MEANS, DATASET.STD_DEVS),\n",
    "image_transform = t.Compose([t.ToImage(), t.ToDtype(torch.float32, scale=True)])\n",
    "mask_transform = t.Compose([t.ToImage(), t.ToDtype(torch.float32, scale=False)])\n",
    "#augmentations = t.Compose([t.Pad(6)])\n",
    "augmentations = None\n",
    "\n",
    "#import pandas as pd\n",
    "#train_df = DATASET.scene_df(**experiment)\n",
    "#train_df = train_df[train_df[\"split\"] == \"train\"]\n",
    "#eval_df = DATASET.tiled_df(**experiment)\n",
    "#eval_df = eval_df.drop(columns = \"tile_name\")\n",
    "#eval_df = eval_df[eval_df[\"split\"] != \"train\"]\n",
    "#dataset_df = pd.concat([train_df, eval_df], axis = 0)\n",
    "#dataset_df = dataset_df[dataset_df[\"split\"] != \"unsup\"]\n",
    "\n",
    "datamodule = ImageDatasetDataModule(\n",
    "    root = PATHS.path / \"inria.h5\",\n",
    "    is_remote = False,\n",
    "    is_streaming = False,\n",
    "    dataset_constructor = DATASET, \n",
    "    # dataframe = dataset_df,\n",
    "    image_transform = image_transform,\n",
    "    target_transform = mask_transform,\n",
    "    common_transform = augmentations,\n",
    "    **experiment\n",
    ")\n",
    "display(datamodule)\n",
    "logger = setup_logger(PATHS.experiments_path, experiment[\"name\"])\n",
    "wandb_logger = setup_wandb_logger(PATHS.experiments_path, experiment[\"name\"])\n",
    "checkpoint = setup_checkpoint(Path(logger.log_dir, \"model_ckpts\"), experiment[\"monitor_metric\"], experiment[\"monitor_mode\"], \"all\") \n",
    "eval_callback = EvaluateSegmentation() if experiment[\"task\"] == \"segmentation\" else EvaluateClassification()\n",
    "reset_dir(LOGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "\n",
    "#from torchvision.models import resnet18, ResNet18_Weights\n",
    "#model = resnet18(weights = ResNet18_Weights.DEFAULT)\n",
    "#model.fc = torch.nn.Linear(512, experiment[\"num_classes\"])\n",
    "\n",
    "#from torchvision.models import alexnet, AlexNet_Weights\n",
    "#model = alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "#model = alexnet(weights=None)\n",
    "#model.classifier[-1] = torch.nn.Linear(4096, experiment.get(\"num_classes\", 10))\n",
    "\n",
    "from segmentation_models_pytorch import Unet\n",
    "model = Unet(experiment[\"model_params\"][\"encoder\"], classes=experiment[\"num_classes\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_CKPT = checkpoint.best_model_path \n",
    "LAST_CKPT = checkpoint.last_model_path\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[checkpoint, eval_callback],\n",
    "    #enable_checkpointing=False,\n",
    "    logger = [logger],\n",
    "    enable_model_summary=False,\n",
    "    #fast_dev_run=True,\n",
    "    #num_sanity_val_steps=0,\n",
    "    max_epochs=2,\n",
    "    check_val_every_n_epoch=1, \n",
    "    #limit_val_batches=10,\n",
    "    #limit_train_batches=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#experiment[\"optimizer_params\"][\"lr\"] =  5e-6 \n",
    "trainer.fit(\n",
    "    model=ClassificationTask(model, **experiment),\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=LAST_CKPT if Path(LAST_CKPT).is_file() else None,\n",
    "    #verbose=False\n",
    ")\n",
    "\n",
    "#trainer.test(\n",
    "    #model=ClassificationTask(model, **experiment),\n",
    "    #datamodule=datamodule,\n",
    "    #ckpt_path=LAST_CKPT if Path(LAST_CKPT).is_file() else None,\n",
    "    #verbose = False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.evaluation import checkpoints_df, plot_checkpoints, plot_checkpoint_attribution\n",
    "plot_checkpoints(LOGS_DIR, experiment[\"monitor_metric\"], checkpoints_df(LOGS_DIR, experiment[\"monitor_metric\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 11 \n",
    "step = 3024 \n",
    "split = \"best\"\n",
    "k = 25 \n",
    "plot_checkpoint_attribution(model, LOGS_DIR, PATHS.path, DATASET, epoch, step, split, k, **experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
